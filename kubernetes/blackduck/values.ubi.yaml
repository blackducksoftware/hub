# Default values for bd.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride:
fullnameOverride:

isKubernetes: true
isAzure: false

imageTag: "2024.10.0"

# Docker registry to pull Black Duck images
registry: docker.io/blackducksoftware

# image pull secret to download the Black Duck images (mostly applicable for air gapped customers).
# array of strings delimited by comma
imagePullSecrets: []

# Expose Black Duck's User Interface
exposeui: true
# possible values are NodePort, LoadBalancer or OpenShift (in case of routes)
exposedServiceType: NodePort
# custom port to expose the NodePort service on
exposedNodePort: ""

# enable Persistent Storage for containers
enablePersistentStorage: true
# it will apply to all PVC's storage class but it can be override at container level
storageClass:

# enable Liveness probe
enableLivenessProbe: true

# enable Init container to check for Postgres connection
enableInitContainer: true

# enable source code upload
enableSourceCodeUpload: false
# days to retain the data
dataRetentionInDays: 180
# maximum size of total source file size in MB
maxTotalSourceSizeinMB: 4000

# enable binary scanner
enableBinaryScanner: false

# if true, enables application level encryption
enableApplicationLevelEncryption: false

# enable artifactory integration
enableIntegration: false

# configure Black Duck to use an Alert instance
# (default alertNamespace is Release.Namespace)
enableAlert: false
alertName: ""
alertNamespace: ""

# enable IPV6
enableIPV6: true

# Maximum number of database connections allowed to each container.
# Note that raising this limit on a heavily loaded system may reduce
# system throughput rather than increase it.
dbPoolMaxActive: 32

# TLS certificate for Black Duck
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-webserver-certificate --from-file=WEBSERVER_CUSTOM_CERT_FILE=tls.crt --from-file=WEBSERVER_CUSTOM_KEY_FILE=tls.key
# tlsCertSecretName: <name>-blackduck-webserver-certificate

# Certificate Authentication Custom CA certificate for Black Duck (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-auth-custom-ca --from-file=AUTH_CUSTOM_CA=ca.crt
# certAuthCACertSecretName: <name>-blackduck-auth-custom-ca

# Proxy certificate for Black Duck (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-proxy-certificate --from-file=HUB_PROXY_CERT_FILE=proxy.crt
# proxyCertSecretName: <name>-blackduck-proxy-certificate

# Proxy password for Black Duck (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-proxy-password --from-file=HUB_PROXY_PASSWORD_FILE=proxy_password_file
# proxyPasswordSecretName: <name>-blackduck-proxy-password

# LDAP password for Black Duck (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-ldap-password --from-file=LDAP_TRUST_STORE_PASSWORD_FILE=ldap_password_file
# ldapPasswordSecretName: <name>-blackduck-ldap-password

# Service Account passwords (old and current) for Generative AI Feature (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-service-genai-passwords --from-file=HUB_SERVICE_GENAI_OLD_PASSWORD_FILE=service_genai_old_password_file --from-file=HUB_SERVICE_GENAI_CURRENT_PASSWORD_FILE=service_genai_cur_password_file
# serviceAccPasswordsSecretName: <name>-blackduck-service-genai-passwords

# JWT Public Key and Private Key for Black Duck (Not Mandatory)
# create a generic secret using the following command
# kubectl create secret generic -n <namespace> <name>-blackduck-jwt-keypair --from-file=JWT_PUBLIC_KEY=public_key_file --from-file=JWT_PRIVATE_KEY=private_key_file
# jwtKeyPairSecretName: <name>-blackduck-jwt-keypair

# Cryptography root seed for application level encryption (Mandatory if ALE is enabled)
# Example: create a generic secret using the following command
# openssl rand -hex 1024 > root_seed

# kubectl create secret generic crypto-root-seed -n <namespace --save-config --dry-run=client --from-file=crypto-root-seed=./root_seed -o yaml | kubectl apply -f -
# delete root_seed # or store this seed in a secure location - don't leave it on the filesystem

# Cryptography previous root seed for application level encryption. Used for key rotation (Not Mandatory)
# Example: create a generic secret using the following command
# Fetch the current root seed making it the previous root seed
# kubectl get secret crypto-backup-seed -n <namespace>-o jsonpath='{.data}' | jq -r '.["crypto-backup-seed"]' | base64 -d > prev_root_seed
# Next create the previous root seed secret from that file
# create secret generic crypto-prev-seed -n <namespace --save-config --dry-run=client --from-file=crypto-prev-seed=./prev_root_seed -o yaml | kubectl apply -f -
# delete prev_root_seed # or store this seed in a secure location - don't leave it on the filesystem
# After setting the previous root seed, you can go onto set a new root seed in order to rotate the root seed.

# Cryptography backup root seed for application level encryption (Not Mandatory)
# Example: create a generic secret using the following command
# openssl rand -hex 1024 > backup_root_seed
# kubectl create secret generic crypto-backup-seed -n <namespace --save-config --dry-run=client --from-file=crypto-backup-seed=./backup_root_seed -o yaml | kubectl apply -f -
# delete backup_root_seed # or store this seed in a secure location - don't leave it on the filesystem
#
# Once the backup root seed has been discovered and processed, it can be removed from the secrets and the system
# will retain it. To cycle the backup seed, just set a backup seed with the new value
#
# Example: delete backup seed after it has been injested
# kubectl delete secret -n <namespace> crypto-backup-seed

# Maximum number of minutes to wait for database schema update scripts to run
maxWaitForSchemaUpdatesMinutes: 240

# This is used to start or stop the hub. Set to Running to start the hub, or Stopped to stop the hub.
status: Running

# Set to true to run a job that will migrate the postgres container data volume to PostgreSQL 11.  Only valid when
# status is set to "Stopped" and postgres.isExternal is "false".
runPostgresMigration: false

# additional environ values that need to be added into Black Duck configuration
# go to templates/configmap.yaml to configure existing environs
# if you are setting the value using set flag in helm command, do --set environs.* = ""; i.e.: --set environs.HUB_POSTGRES_CONNECTION_ADMIN="blackduck@pg-server-name"
environs:
  BLACKDUCK_CFSSL_PORT: "8888"
  BROKER_USE_SSL: "yes"
  HTTPS_VERIFY_CERTS: "yes"
  RABBIT_MQ_PORT: "5671"
  RABBITMQ_DEFAULT_VHOST: "protecodesc"
  RUN_SECRETS_DIR: "/tmp/secrets"
  SCANNER_CONCURRENCY: "1"
  STRUCTURED_LOGGING: "false"

postgres:
  # override the docker registry at container level
  registry:
  # false for running Postgres as a container and true for using External Postgres database
  isExternal: true
  # required only for external postgres, for postgres as a container, it will point to <name>-blackduck-postgres
  host:
  port: 5432
  # Maximum number of parameters per query; do not change unless directed by Synopsys technical support
  internalParameterLimit: 12000
  externalParameterLimit: 12000
  # path to postgresql initialization script
  pathToPsqlInitScript: "external-postgres-init.pgsql"
  # NOTE: do not change usernames when using postgres as a container; only configure if using external database
  # we only use adminUsername and adminPassword for initializing and set up of the database; containers use 'blackduck_user' for operations
  adminUserName: postgres
  userUserName: blackduck_user
  # TODO: CHANGE THESE
  adminPassword: testPassword
  userPassword: testPassword
  # Controls whether Black Duck uses SSL for external Postgres connections
  ssl: true
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  imageTag: 14-1.22_ubi9.4
  ### Postgres main data volume
  # pvc related parameters for postgres container. set if you want to create your own PVC
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "150Gi"
  # PVC storage class name
  storageClass: ""
  # existing persistent volume name backing the PVC
  volumeName: ""
  ### Small volume for postgresql configuration management
  # Set if you want to create your own PVC
  confPersistentVolumeClaimName:
  # PVC claim size
  confClaimSize: "5Mi"
  # PVC storage class name
  confStorageClass: ""
  # existing persistent volume name backing the PVC
  confVolumeName: ""
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

postgresUpgrader:
  # override the docker registry at container level
  registry:
  securityContext: {}
  podSecurityContext: {}
  resources: {}
  imageTag: 14-1.4_ubi9.4
  imagePullPolicy: IfNotPresent

postgresWaiter:
  # override the docker registry at container level
  registry:
  securityContext: {}
  resources: {}
  imageTag: 1.0.14
  imagePullPolicy: IfNotPresent

postgresInit:
  # To override the image registry or tag, set postgres.registry or postgres.imageTag
  # (even if using external PostgreSQL).
  imagePullPolicy: IfNotPresent
  resources:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "300m"
      memory: "512Mi"

authentication:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  # pvc related parameters for authentication container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "2Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  # jvm max heap memory -Xmx [NOTE: THIS SHOULD BE SET RELATIVELY TO resources.limits.memory (specifically less than resources.limits.memory)]
  hubMaxMemory: "512m"
  resources:
    limits:
      memory: "1024Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

binaryscanner:
  # override the docker registry at container level
  registry: "docker.io/sigsynopsys"
  # override the global imageTag
  imageTag: 2024.9.1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  resources:
    limits:
      cpu: "1000m"
      memory: "4096Mi"
    requests:
      cpu: "500m"
      memory: "4096Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

bomengine:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  dbPoolMaxActive:
  affinity: {}
  nodeSelector: {}
  podSecurityContext: {}
  securityContext: {}
  tolerations: []
  replicas: 1
  resources: {}
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

cfssl:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 1.0.30_ubi9.4
  # pvc related parameters for cfssl container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "2Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  resources:
    limits:
      memory: "640Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

documentation:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  resources:
    limits:
      memory: "512Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

jobrunner:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  # NOTE: threads & prefetch should be set proportionately to hubMaxMemory
  maxPeriodicThreads: 3
  maxPeriodicPrefetch: 2
  maxOndemandThreads: 4
  maxOndemandPrefetch: 4
  # jvm max heap memory -Xmx [NOTE: THIS SHOULD BE SET RELATIVELY TO resources.limits.memory (specifically less than resources.limits.memory)]
  hubMaxMemory: "3072m"
  resources: {}
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

rabbitmq:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 1.2.41_ubi9.4
  # pvc related parameters for rabbitmq container
  persistentVolumeClaimName:
    # PVC claim size
  claimSize: "2Gi"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  resources:
    limits:
      memory: "1024Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

redis:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  tlsEnabled: false
  maxTotal: 128
  maxIdle: 128
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  resources:
    limits:
      memory: "2048Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

registration:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  # pvc related parameters for registration container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "2Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  hubMaxMemory: "640m"
  resources:
    requests:
      cpu: "1000m"
    limits:
      memory: "1024Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

rlservice:
  enabled: false
  # override the docker registry at container level
  registry:
  # no. of service replicas
  replicas: 1
  # override the global imageTag
  imageTag: 2024.10.0
  # pvc related parameters for rlservice container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "2Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  hubMaxMemory: "5530m"
  resources:
    requests:
      cpu: "2000m"
      memory: "6144Mi"
    limits:
      cpu: "2000m"
      memory: "6144Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

scan:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  # jvm max heap memory -Xmx [NOTE: THIS SHOULD BE SET RELATIVELY TO resources.limits.memory (specifically less than resources.limits.memory)]
  hubMaxMemory: "2048m"
  resources: {}
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

storage:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  # PVC related parameters for storage cache container.
  # If you want to use already existing persistance volume for blackduck tools & reports storage, use this option to configure the existing pvc name.
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "100Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  # jvm max heap memory -Xmx [NOTE: THIS SHOULD BE SET RELATIVELY TO resources.limits.memory (specifically less than resources.limits.memory)]
  hubMaxMemory: "1024m"
  resources: {}
  podAnnotations: {}
  providers: []
  imagePullPolicy: IfNotPresent

webapp:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  # pvc related parameters for webapp container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "2Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  # jvm max heap memory -Xmx [NOTE: THIS SHOULD BE SET RELATIVELY TO resources.limits.memory (specifically less than resources.limits.memory)]
  hubMaxMemory: "2048m"
  resources: {}
  imagePullPolicy: IfNotPresent

logstash:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 1.0.39_ubi9.4
  # pvc related parameters for logstash container. set if you want to create your own PVC (used for migration)
  persistentVolumeClaimName:
  # PVC claim size
  claimSize: "20Gi"
  # PVC storage class name
  storageClass:
  # existing persistent volume name backing the PVC
  volumeName:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # podSecurityContext is set in webapp above
  securityContext: {}
  hubMaxMemory: "2048m"
  resources:
    limits:
      memory: "2560Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

webserver:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

datadog:
  enabled: false
  # override the docker registry at container level
  registry:
  imageTag: "1.0.16"
  imagePullPolicy: IfNotPresent

matchengine:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  dbPoolMaxActive:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podSecurityContext: {}
  securityContext: {}
  replicas: 1
  hubMaxMemory: "4096m"
  resources:
    limits:
      memory: "4608Mi"
  podAnnotations: {}
  imagePullPolicy: IfNotPresent

integration:
  # override the docker registry at container level
  registry:
  # override the global imageTag
  imageTag: 2024.10.0_ubi9.4
  dbPoolMaxActive:
  affinity: {}
  nodeSelector: {}
  podSecurityContext: {}
  securityContext: {}
  tolerations: []
  replicas: 1
  maxRamPercentage: 90
  resources:
    limits:
      cpu: "1000m"
      memory: "5120Mi"
    requests:
      cpu: "500m"
      memory: "5120Mi"
  podAnnotations: {}
  rateMetering:
    artifactoryIntegration: {}
  imagePullPolicy: IfNotPresent


metrics:
  enabled: false
